{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["4nB4WbeB-9Po","hpI3rqNIHnVu"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Introdution"],"metadata":{"id":"4nB4WbeB-9Po"}},{"cell_type":"markdown","source":["Until now, we've looked at the fundamentals of preprocessing and the use of the bag-of-words approach to model topics in LDA (Latent Dirichlet Allocation).\n","\n","Now, let's expand on some concepts and revisit, for example, the use of the bag-of-words approach to compare documents. Imagine that we have two documents, and we want to determine whether they discuss the same topic, based on the assumption that if both documents share the same terms, they likely cover the same subject.\n","\n","With this in mind, we will use the concept of document vectors in the bag-of-words model, which can be explained as representing each document as a vector of word counts or frequencies within a fixed vocabulary. In this vector representation, each dimension corresponds to a specific word in the vocabulary, and the value in each dimension represents how often that word appears in the document.\n","\n"],"metadata":{"id":"5BgoK0VU_CQ9"}},{"cell_type":"markdown","source":["#Vectors"],"metadata":{"id":"hpI3rqNIHnVu"}},{"cell_type":"markdown","source":["By transforming documents into these numerical vectors, we can easily compare them using similarity metrics, such as cosine similarity. If two document vectors have high similarity, this suggests that the documents share a significant amount of overlapping content, indicating a related subject. The bag-of-words approach thus provides a straightforward way to quantify the content of documents for comparisons and further analysis in natural language processing tasks.\n","\n","To compare documents using the Bag of Words approach with CountVectorizer from scikit-learn, we can follow these steps:\n","\n","1. Transform each document into a word count vector using CountVectorizer.\n","2. Calculate the similarity between the vectors. One of the most common methods is cosine similarity."],"metadata":{"id":"IJPzmxeZHfrw"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Exemplo de documentos\n","doc1 = \"Eu amo aprender sobre processamento de linguagem natural\"\n","doc2 = \"O processamento de linguagem natural é fascinante e cheio de possibilidades\"\n","\n","# Lista de documentos\n","documents = [doc1, doc2]\n","\n","# Inicializa o CountVectorizer\n","vectorizer = CountVectorizer()\n","\n","\n","# Converte os documentos para a representação bag-of-words\n","X = vectorizer.fit_transform(documents)\n","\n","# Calcula a similaridade do cosseno entre os vetores dos documentos\n","cosine_sim = cosine_similarity(X[0], X[1])\n","\n","# Exibe a similaridade entre doc1 e doc2\n","print(\"Similaridade entre doc1 e doc2:\", cosine_sim[0][0])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mY35HdwcBx89","outputId":"715fc44d-c8c6-411a-9b7c-24e18e45ca55","executionInfo":{"status":"ok","timestamp":1731500614140,"user_tz":0,"elapsed":10308,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Similaridade entre doc1 e doc2: 0.5590169943749475\n"]}]},{"cell_type":"markdown","source":["Here is an example with three documents: two on the same topic (natural language processing) and a third on a different subject (weather). Using the above code calculate the cosine similarity."],"metadata":{"id":"gxxh7DrcCEte"}},{"cell_type":"code","source":["# Exemplo de documentos\n","doc1 = \"O processamento de linguagem natural é fascinante e cheio de possibilidades.\"\n","doc2 = \"A área de processamento de linguagem natural envolve a análise de textos e fala.\"\n","doc3 = \"Hoje o clima está ensolarado com algumas nuvens.\"\n","\n"],"metadata":{"id":"6gmv07xpCIRu","executionInfo":{"status":"ok","timestamp":1731500625971,"user_tz":0,"elapsed":257,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Print the obtained vectors and explain the vector in terms of which words are in each position of the vector."],"metadata":{"id":"f5dkCYkmHyZH"}},{"cell_type":"code","source":[],"metadata":{"id":"fVwCbI4XDipf","executionInfo":{"status":"ok","timestamp":1731500637121,"user_tz":0,"elapsed":264,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["In what range does cosine similarity work?"],"metadata":{"id":"MxZyvs-GKtRb"}},{"cell_type":"code","source":[],"metadata":{"id":"lL7r8mZPLGPl","executionInfo":{"status":"ok","timestamp":1731500641922,"user_tz":0,"elapsed":252,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["In the example above, we used cosine similarity as a metric, but there are other alternatives that can be employed to compare vectors generated by the bag-of-words model. Some of these include Euclidean distance, which measures the straight-line distance between two points in a multi-dimensional space, and Manhattan distance, which calculates the sum of the absolute differences across each dimension. Additionally, the Jaccard similarity coefficient can be used to measure the overlap between two sets, making it particularly useful when focusing on the presence or absence of words rather than their frequency. Each metric has its own advantages depending on the context and desired outcome of the comparison, as some are more sensitive to vector magnitude, while others emphasize the direction or relative frequency of terms."],"metadata":{"id":"DifqMc2TIfJL"}},{"cell_type":"markdown","source":["# Sentiment Analysis and Text Categorization"],"metadata":{"id":"fuO5jYp0MXRa"}},{"cell_type":"markdown","source":["\n","\n","**Sentiment analysis** is a type of text classification that involves determining the sentiment or emotional tone behind a piece of text. This can be categorized as either positive, negative, or neutral, though more granular classifications (such as very positive, somewhat negative, etc.) are also possible. The main goal of sentiment analysis is to analyze and extract subjective information from text, such as opinions, reviews, or social media posts.\n","\n","## What is a Sentiment?\n","\n","A **sentiment** refers to the underlying emotional state or opinion expressed in a piece of text. The sentiment can be for example:\n","\n","0. **Negative Sentiment**: Expresses an unfavorable, unhappy, or disapproving tone.  \n","   Example: \"The service at the restaurant was terrible. I'll never go again.\"\n","\n","1. **Positive Sentiment**: Expresses a favorable, happy, or approving tone.  \n","   Example: \"I absolutely loved the movie! It was fantastic!\"\n","\n","\n","\n","## Naive Bayes for Sentiment Analysis\n","\n","**Naive Bayes classifiers** are a popular machine learning approach used for sentiment analysis because of their simplicity and effectiveness, especially with text data.\n","\n","A **Naive Bayes classifier** is based on Bayes' Theorem, which calculates the probability of a given sentiment (category) based on the words present in the text. The key assumption of Naive Bayes is that the features (words) in the text are **independent** of each other, which is why it's called \"naive.\" Despite this simplifying assumption, it often performs surprisingly well for text classification tasks.\n","\n","### Steps in Naive Bayes for Sentiment Analysis:\n","\n","1. **Training**:\n","   - The model is trained using a labeled dataset that includes text and its corresponding sentiment label (positive, negative, or neutral). For instance, a set of apps reviews labeled as positive or negative.\n","\n","2. **Feature Extraction**:\n","   - In text data, features are usually individual words or sequences of words. For each word, the model calculates the probability of the word appearing in texts with a certain sentiment label.\n","\n","3. **Calculating Probabilities**:\n","   - The Naive Bayes classifier computes the probability of a sentiment label given the words in a new text.\n","\n","4. **Classification**:\n","   - When a new text is provided, the model calculates the probabilities for each sentiment class (positive, negative, neutral). It then assigns the sentiment with the highest probability to the text.\n","\n","### Example:\n","\n","Let's say we have a training set with the following data:\n","\n","- \"I love this product\" (positive)\n","- \"This is the worst purchase ever\" (negative)\n","- \"The product is okay, but not great\" (neutral)\n","\n","For the phrase \"love this product\":\n","- The word \"love\" might have a higher probability of occurring in positive reviews, while words like \"worst\" might be more likely in negative reviews.\n","\n","The Naive Bayes classifier uses this statistical approach to make predictions on new, unseen text, based on the probabilities calculated from the training data.\n","\n","In summary, sentiment analysis is a form of text categorization that focuses on identifying the emotional tone of text. Naive Bayes classifiers are commonly used in this task due to their efficiency in handling text data and their ability to classify based on word probabilities.\n","\n","More information here:\n","https://www.youtube.com/watch?v=O2L2Uv9pdDA&t=639s\n"],"metadata":{"id":"xQX1gaTmRQQy"}},{"cell_type":"markdown","source":["Now that we understand how Naive Bayes works, let's implement a model in Python to classify app reviews. The data was provided in a CSV file. Download the data and load it. The libraries we will use are:"],"metadata":{"id":"8MJmJpQheCA6"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"gG8P6UNIpIvM","executionInfo":{"status":"ok","timestamp":1731500654371,"user_tz":0,"elapsed":239,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"outputs":[],"source":["#import pandas para trabalhar com dataframes\n","import pandas as pd\n","\n","#import para fazer divisao do corpus em train e test\n","from sklearn.model_selection import train_test_split\n","\n","#library for convert a collection of text documents to a matrix of token counts.\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","#call da library naive bayes\n","from sklearn.naive_bayes import MultinomialNB\n","\n","import numpy as np\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","\n","# import counter class from collections module\n","from collections import Counter"]},{"cell_type":"markdown","source":["Load the dataset in pandas directly from the csv file."],"metadata":{"id":"7ev88DZFeIhv"}},{"cell_type":"code","source":[],"metadata":{"id":"DXeMHsdNeHlk","executionInfo":{"status":"ok","timestamp":1731500659750,"user_tz":0,"elapsed":4,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["What characteristics (size, columns...) does the dataset have?"],"metadata":{"id":"-mM4B_b1eawd"}},{"cell_type":"code","source":[],"metadata":{"id":"3cwG96VIeo17","executionInfo":{"status":"ok","timestamp":1731500663095,"user_tz":0,"elapsed":235,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["What is the distribution of the labels in the dataset?"],"metadata":{"id":"AifhddbYevfx"}},{"cell_type":"code","source":[],"metadata":{"id":"UigpFE8L0gem","executionInfo":{"status":"ok","timestamp":1731500673500,"user_tz":0,"elapsed":279,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Preprocess the data in a way that only keeps the columns in the dataframe that contain the text 'raw' and the annotated labels."],"metadata":{"id":"pdXstG1XfmmH"}},{"cell_type":"code","source":[],"metadata":{"id":"u6dZ9N4Vpt0G","executionInfo":{"status":"ok","timestamp":1731500679815,"user_tz":0,"elapsed":240,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Now that you have the appropriate corpus, split it into test documents and training documents. Use `train_test_split` to do this."],"metadata":{"id":"rif9kMcpiEQN"}},{"cell_type":"code","source":[],"metadata":{"id":"_K6CWAH-tuof","executionInfo":{"status":"ok","timestamp":1731500687098,"user_tz":0,"elapsed":245,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["The function train_test_split splits the data into 4 parts. Explain each of these parts."],"metadata":{"id":"1uE3yIMjjWof"}},{"cell_type":"code","source":[],"metadata":{"id":"OGnBfUChjnKE","executionInfo":{"status":"ok","timestamp":1731500699374,"user_tz":0,"elapsed":268,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["We have been working with vectors from scratch until now. But to simplify today's analysis, I created vectors from the review data using `CountVectorizer`. I removed the stopwords."],"metadata":{"id":"I8dH1_Bsip9_"}},{"cell_type":"code","source":[],"metadata":{"id":"Gu-YiLIMqPyB","executionInfo":{"status":"ok","timestamp":1731500704606,"user_tz":0,"elapsed":382,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Characterize the generated vectors. What is the shape of these vectors?"],"metadata":{"id":"ZTjx94m0iygd"}},{"cell_type":"code","source":[],"metadata":{"id":"MDmcLH-1u8FQ","executionInfo":{"status":"ok","timestamp":1731500707277,"user_tz":0,"elapsed":328,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Now that you know how to create the vectors, create the vectors only for the test data (reviews) generated by the train_test_split."],"metadata":{"id":"0Z2GbylXj_B6"}},{"cell_type":"code","source":[],"metadata":{"id":"JDJ3UYIlurvd","executionInfo":{"status":"ok","timestamp":1731500713746,"user_tz":0,"elapsed":314,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Use the `MultinomialNB` function to fit the vectors created from the test data with the labeled annotations of each review."],"metadata":{"id":"G6lJvmypkbgC"}},{"cell_type":"code","source":[],"metadata":{"id":"xkOud2nlqrTE","executionInfo":{"status":"ok","timestamp":1731500724097,"user_tz":0,"elapsed":282,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Use `model.score` to measure the performance of your classification model."],"metadata":{"id":"981kt36Tl7sl"}},{"cell_type":"code","source":[],"metadata":{"id":"DfgSDYt5qzWy","executionInfo":{"status":"ok","timestamp":1731500727933,"user_tz":0,"elapsed":270,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["How did your model perform? Now, create a confusion matrix to analyze what your model predicts best. The negative or positive labels?"],"metadata":{"id":"zVX1T4HZm9J4"}},{"cell_type":"code","source":[],"metadata":{"id":"U1KCAg_55HNt","executionInfo":{"status":"ok","timestamp":1731500763052,"user_tz":0,"elapsed":248,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Consider the new review cases in Dataflame below"],"metadata":{"id":"Q0VWrb-bna5y"}},{"cell_type":"code","source":[],"metadata":{"id":"i0JCN0Wrwq8J","executionInfo":{"status":"ok","timestamp":1731500910567,"user_tz":0,"elapsed":308,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Make the prediction for these new cases proposed by the new dataframe."],"metadata":{"id":"cpOI-YO8oQFa"}},{"cell_type":"code","source":[],"metadata":{"id":"XWG905Yu1Sus","executionInfo":{"status":"ok","timestamp":1731500913048,"user_tz":0,"elapsed":274,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["How did your model perform for these new cases?"],"metadata":{"id":"QjPETegTobm-"}},{"cell_type":"code","source":[],"metadata":{"id":"28OQmL1Vob7R","executionInfo":{"status":"ok","timestamp":1731500918626,"user_tz":0,"elapsed":373,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Consider the image below and discuss with the teacher about the metrics of precision and recall."],"metadata":{"id":"5zTCfGxAgX-K"}},{"cell_type":"markdown","source":["![Texto alternativo](https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg)"],"metadata":{"id":"tpAkPnSSgfDR"}},{"cell_type":"markdown","source":["Calculate the precision and recall of the model you just created."],"metadata":{"id":"r0LyPoZqgitZ"}},{"cell_type":"code","source":[],"metadata":{"id":"kc7unl3AgYRs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Consider the formulas below:\n","\n","The **F1-Score** is an evaluation metric that combines **Precision** and **Recall** into a single measure, using the harmonic mean of these values. The mathematical formula for the F1-Score is as follows:\n","\n","$$\n","F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n","$$\n","\n","\n","Where:\n","\n","- **Precision** is the proportion of true positives among all examples predicted as positive.\n","\n","$$\n","\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n","$$\n","\n","- **Recall** is the proportion of true positives among all examples that are actually positive.\n","\n","$$\n","\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n","$$\n","\n","Thus, the F1-Score evaluates model performance by balancing precision and recall, making it especially useful in scenarios with imbalanced classes.\n"],"metadata":{"id":"jqIzSgiSgj8k"}},{"cell_type":"markdown","source":["Calculate the F1 score for the model you just created."],"metadata":{"id":"Pi1Wssp8gq6Z"}},{"cell_type":"code","source":[],"metadata":{"id":"3XfeYcoBgm05"},"execution_count":null,"outputs":[]}]}