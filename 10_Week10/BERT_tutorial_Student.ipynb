{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPZp8jOLxL8mqryKgOhFSdl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Hugging face and loading\n","\n","**Introduction to BERT and Hugging Face Models**  \n","\n","BERT (Bidirectional Encoder Representations from Transformers) is a groundbreaking model introduced by Google that has significantly advanced the field of Natural Language Processing (NLP). Its core innovation lies in its bidirectional training of transformers, allowing it to understand context from both directions of a sentence. This approach makes BERT exceptionally effective in tasks like sentiment analysis, text classification, question answering, and more.  \n","\n","One of the most accessible ways to work with BERT is through Hugging Face, a platform that hosts a vast repository of pre-trained models. Hugging Face provides tools and resources that simplify the implementation of NLP tasks using BERT and its derivatives. With just a few lines of code, users can access, fine-tune, and deploy state-of-the-art models for various applications.  \n","\n","By leveraging the Hugging Face Transformers library, researchers and developers can explore a wide range of pre-trained models, from base BERT to specialized variations fine-tuned for specific tasks. The platform also encourages collaboration and innovation by enabling users to share their models and use community-contributed resources.  \n","\n","In this tutorial, we will explore how to use BERT via the Hugging Face ecosystem, covering everything from loading pre-trained models to customizing them for your unique use case. Let’s dive in!"],"metadata":{"id":"vMedLGSfYRX6"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"fHHUDcEKGvFz"}},{"cell_type":"markdown","source":["**Exercise 1: Exploring Hugging Face Models**  \n","\n","1. Access the website [https://huggingface.co/](https://huggingface.co/).  \n","2. Explore the available models and identify one or more models you are interested in using.  \n","3. Investigate how to import the selected models using the Hugging Face library or APIs.  \n","4. Discuss with your instructor the steps and best practices for importing and utilizing these models in your project.  "],"metadata":{"id":"P6aXZtr-Gz9F"}},{"cell_type":"code","source":[],"metadata":{"id":"jy9uma0tZpAe","executionInfo":{"status":"ok","timestamp":1733932790499,"user_tz":0,"elapsed":201,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Embeddings using BERT"],"metadata":{"id":"6VWEBaLscFZI"}},{"cell_type":"markdown","source":["**Introduction to Word Embeddings**  \n","\n","Word embeddings are a fundamental concept in Natural Language Processing (NLP) that transform words into numerical representations. These representations capture semantic meanings and relationships between words, making them essential for many NLP tasks such as language modeling, sentiment analysis, and machine translation.  \n","\n","Traditional methods like one-hot encoding represented words as sparse vectors, which lacked meaningful relationships and scalability. In contrast, word embeddings represent words as dense, low-dimensional vectors. This approach preserves linguistic context and enables models to understand similarities between words based on their usage in language. For example, embeddings of words like *king* and *queen* or *dog* and *cat* will be close to each other in the embedding space.  \n","\n","Popular techniques for generating word embeddings include Word2Vec, GloVe, and FastText, which learn these representations from large text corpora. More advanced models like BERT and GPT use contextualized embeddings, which dynamically adjust the vector representation of a word based on its context in a sentence.  \n","\n","In this part of the tutorial, we will delve into the basics of word embeddings, explore their mathematical properties, and see how they form the building blocks for more complex NLP models. Let’s get started!  "],"metadata":{"id":"boZjMvAtHoZj"}},{"cell_type":"markdown","source":["## Word Embeddings"],"metadata":{"id":"yFxFphTxcS64"}},{"cell_type":"markdown","source":["**Exercise 2: Extracting Word Embeddings**  \n","\n","1. Obtain the embeddings for the word *\"bank\"* from the sentence *\"My money is in the bank.\"*.  \n","2. Specify which model you used to extract the embeddings (e.g., BERT, DistilBERT, or another Hugging Face model).  \n","3. Identify and explain the required inputs and outputs for the model to generate the embeddings.  \n","   - What preprocessing steps were necessary for the input sentence?  \n","   - What format did the model return for the embeddings (e.g., vector size, structure)?  \n","\n","Discuss your findings and approach with your instructor.  "],"metadata":{"id":"K7K82JvJHriO"}},{"cell_type":"code","source":[],"metadata":{"id":"pxIU3nINZ1K_","executionInfo":{"status":"ok","timestamp":1733932790819,"user_tz":0,"elapsed":4,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Words in polysemy"],"metadata":{"id":"3Gg2h9SYcVpg"}},{"cell_type":"markdown","source":["**Exercise 3: Exploring Polysemy with Word Embeddings**  \n","\n","1. Use the same model from Exercise 2 to generate embeddings for the word *\"bank\"* in the sentence *\"The boat is stuck on the bank.\"*.  \n","2. Compare the embeddings of the word *\"bank\"* from Exercise 2 (*\"My money is in the bank.\"*) with those obtained in this exercise.  \n","3. Measure the similarity between the two embeddings using cosine similarity.  \n","   - Are the embeddings identical or different?  \n","   - Discuss how the model handles polysemy (words with multiple meanings) based on the context provided by the sentences.  \n","\n","Share your findings and insights with your instructor."],"metadata":{"id":"XBcavUCuJZDJ"}},{"cell_type":"code","source":[],"metadata":{"id":"Fi03mv29cjoG","executionInfo":{"status":"ok","timestamp":1733932790819,"user_tz":0,"elapsed":3,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 4: Comparing Contextually Similar Words**  \n","\n","1. Use the same model as in previous exercises to generate the embedding for the word *\"safe\"* in the sentence *\"My money is in the safe.\"*.  \n","2. Compare the embedding of *\"safe\"* with the embedding of *\"bank\"* obtained from the sentence *\"My money is in the bank.\"* (Exercise 2).  \n","3. Calculate the cosine similarity between the embeddings of *\"bank\"* and *\"safe\"*.  \n","   - Are the vectors similar?  \n","   - Discuss how the model interprets contextually similar but distinct words.  \n","\n","Present your observations and discuss the implications with your instructor."],"metadata":{"id":"fe2Q8MZVJcs1"}},{"cell_type":"code","source":[],"metadata":{"id":"MPykVDsGhIWw","executionInfo":{"status":"ok","timestamp":1733932790819,"user_tz":0,"elapsed":3,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sentence embeddings"],"metadata":{"id":"U8V5V2s7chQu"}},{"cell_type":"markdown","source":["**Introduction to Sentence Embeddings**  \n","\n","Sentence embeddings are numerical representations of entire sentences, capturing their semantic meaning and context. Unlike word embeddings, which focus on individual words, sentence embeddings aim to encode the overall message and relationships between words in a sentence. This makes them essential for tasks like text similarity, sentiment analysis, and machine translation.  \n","\n","Models like Sentence-BERT, Universal Sentence Encoder (USE), and others generate sentence embeddings by processing text at the sentence level, often considering the relationships between words and phrases. These embeddings allow for efficient comparisons between sentences, enabling tasks such as clustering, semantic search, and paraphrase detection.  \n","\n","In this tutorial, we will explore how to generate and use sentence embeddings, analyzing their properties and applications in real-world NLP tasks."],"metadata":{"id":"NUk_v2LUK2Vq"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"etAaTioSM1M0"}},{"cell_type":"markdown","source":["**Exercise 5: Generating Sentence Embeddings**  \n","\n","1. Use the model's `pooler_output` to extract the sentence embedding for the sentence *\"I love Portugal\"*.  \n","2. Identify and explain the steps required to process the input and retrieve the embedding.  \n","3. Discuss the dimensions and structure of the resulting sentence embedding.  "],"metadata":{"id":"SboOE2oiM3E1"}},{"cell_type":"code","source":[],"metadata":{"id":"WaRxjZHFcj5B","executionInfo":{"status":"ok","timestamp":1733932790819,"user_tz":0,"elapsed":3,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 6: Comparing Sentence Embeddings**  \n","\n","1. Generate the sentence embedding for *\"I am in love with Portuguese lands\"* using the same method as in Exercise 5, extracting the embedding from `pooler_output`.  \n","2. Compare this embedding with the one generated for *\"I love Portugal\"* in Exercise 5.  \n","3. Use cosine similarity to measure how similar the embeddings are.  \n","   - Are the embeddings close in the semantic space?  \n","   - Discuss how the model captures the similarity in meaning between the two sentences despite differences in wording.  "],"metadata":{"id":"Y4KLSBwHNI9q"}},{"cell_type":"code","source":[],"metadata":{"id":"sore4NWtgddh","executionInfo":{"status":"ok","timestamp":1733932791121,"user_tz":0,"elapsed":304,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 7: Comparing Sentence Embeddings with a New Sentence**  \n","\n","1. Generate the sentence embedding for *\"Pillow fights should be banned.\"* using the same method as in Exercises 5 and 6, extracting the embedding from `pooler_output`.  \n","2. Compare this new embedding with the ones generated for *\"I love Portugal\"* and *\"I am in love with Portuguese lands\"*.  \n","3. Use cosine similarity to measure how similar the embeddings are to each other.  \n","   - How do the embeddings for *\"Pillow fights should be banned\"* compare to those of the previous sentences?  \n","   - Discuss how the model captures semantic differences between the sentences.  \n","\n","Discuss your findings and insights with your instructor."],"metadata":{"id":"RU5NJcgANm1O"}},{"cell_type":"code","source":[],"metadata":{"id":"4atJ14-ttxTj","executionInfo":{"status":"ok","timestamp":1733932791121,"user_tz":0,"elapsed":5,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 8: Using SBERT for Sentence Similarity Comparison**  \n","\n","1. The results from the previous exercise may not have been as effective because the `pooler_output` uses the [CLS] token vector, which might not be optimal for some tasks. However, there is a model specifically trained for Semantic Textual Similarity (STS).  \n","2. Access [https://sbert.net/](https://sbert.net/) and explore the available models.  \n","3. Use the model *'all-MiniLM-L6-v2'* from SBERT to generate sentence embeddings for the sentences:  \n","   - *\"I love Portugal\"*  \n","   - *\"I am in love with Portuguese lands\"*  \n","   - *\"Pillow fights should be banned.\"*  \n","4. Compare the sentence embeddings again using cosine similarity.  \n","   - How do the results differ when using the SBERT model?  \n","   - Discuss the improvement in similarity measurement compared to using the [CLS] token vector from the previous model.  "],"metadata":{"id":"dxgwnAwnN4ZY"}},{"cell_type":"code","source":[],"metadata":{"id":"p5olFA6Q0a5F","executionInfo":{"status":"ok","timestamp":1733932791121,"user_tz":0,"elapsed":5,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Token `[CLS]` in the Original BERT\n","\n","**In the Original BERT Model**:\n","- The `[CLS]` token is used as a special marker added at the beginning of a sequence.\n","- After passing through the network, the embedding associated with `[CLS]` is designed to capture the global information of the sentence. It is frequently used for tasks like sentence classification (e.g., sentiment analysis).\n","- However, the `[CLS]` vector in BERT is not optimized to directly compare sentences or measure semantic similarity. It was fine-tuned on general pre-training tasks, such as masked language modeling and next sentence prediction.\n","\n","**SBERT Goes Beyond `[CLS]`**:\n","- SBERT performs fine-tuning specifically to capture the semantics of entire sentences.\n","- It uses pooling to combine information from all the words in the sentence, rather than relying solely on the `[CLS]` embedding.\n","- SBERT creates embeddings better suited for calculations like cosine similarity between sentences, whereas `[CLS]` vectors from BERT are less consistent for this type of comparison.\n","\n"],"metadata":{"id":"8yjgOAwM0AHO"}},{"cell_type":"markdown","source":["How about Document embeddings?"],"metadata":{"id":"qsOXPWLP2E2n"}},{"cell_type":"markdown","source":["# Translation BERT and Text-to-text"],"metadata":{"id":"uTXRZPahZpZQ"}},{"cell_type":"markdown","source":["BERT (Bidirectional Encoder Representations from Transformers) has significantly advanced the field of Natural Language Processing (NLP), and its capabilities extend to machine translation. While BERT is not specifically designed for translation tasks, its deep understanding of context and bidirectional nature allows it to contribute to translation models. By capturing rich semantic and syntactic information from both directions of a sentence, BERT helps improve the quality of translations by offering more accurate context for word choices, phrase structures, and sentence meanings.\n","In translation tasks, BERT can be used in conjunction with other models like MarianMT or mBART, which are trained specifically for translation, to enhance context understanding and to create more fluent and precise translations. It can also help in tasks such as zero-shot translation, where the model is capable of translating between language pairs it has not explicitly been trained on, thanks to its strong contextual embeddings.\n","In this part of the tutorial, we will explore how BERT can assist in translation tasks and how to integrate it with other translation models for better performance."],"metadata":{"id":"qe4cb2mROi9Z"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"EUfUqeh9RCxp"}},{"cell_type":"markdown","source":["**Exercise 9: Using T5 for Machine Translation**  \n","\n","1. Load the model from Hugging Face: [https://huggingface.co/google-t5/t5-base](https://huggingface.co/google-t5/t5-base).  \n","2. Explore and understand how the T5 model works, particularly in the context of translation tasks.  \n","3. Translate the following text from English to German using the T5 model:  \n","   *\"The house is wonderful.\"*  \n","4. Discuss the model's input and output format, and explain how T5 handles translation tasks."],"metadata":{"id":"pCP191eYRg9c"}},{"cell_type":"code","source":[],"metadata":{"id":"nlU1Slph4B39","executionInfo":{"status":"ok","timestamp":1733932791121,"user_tz":0,"elapsed":5,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 10: Testing Text-to-Text Paradigm for Translation**  \n","\n","1. Using the T5 model from Exercise 9, apply the Text-to-Text paradigm to translate the same sentence *\"The house is wonderful.\"* into other languages of your choice (e.g., Spanish, French, Italian).  \n","2. For each translation, ensure the correct format is used, where the model receives the task as a text input (e.g., \"translate English to Spanish: The quick brown fox jumps over the lazy dog.\") and generates the output in the target language.  \n","3. Discuss how the Text-to-Text paradigm allows the model to handle multiple translation tasks seamlessly by simply changing the input task description.  \n","\n","Share your results and observations with your instructor."],"metadata":{"id":"NDjc1kiOSRwE"}},{"cell_type":"code","source":[],"metadata":{"id":"6l7IiBTK4GyS","executionInfo":{"status":"ok","timestamp":1733932791121,"user_tz":0,"elapsed":5,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Paradigma Text-to-Text:\n","\n","- **Translation**: Input: `translate English to French: How are you?` -> Output: `Comment ça va?`\n","- **Summarization**: Input: `summarize: The article discusses...` -> Output: `Key points are...`\n","- **Question and Answering**: Input: `question: Who wrote Hamlet? context: Hamlet was written by Shakespeare.` -> Output: `Shakespeare`\n"],"metadata":{"id":"YlFtpUbP6FP4"}},{"cell_type":"markdown","source":["# Sentiment Analysis"],"metadata":{"id":"hhp5Ka1SZtbR"}},{"cell_type":"markdown","source":["Sentiment analysis involves determining the emotional tone or opinion expressed in a piece of text, such as whether a product review is positive, negative, or neutral. BERT’s bidirectional nature enables it to understand context in a sentence more effectively than traditional models, making it especially well-suited for tasks like sentiment analysis.\n","BERT captures nuanced meaning by considering both the words before and after a given token, allowing it to understand subtle sentiment shifts and contextual cues that simpler models might miss. For example, it can distinguish between a positive sentence like \"I love this movie!\" and a negative one like \"I hate this movie!\" by grasping the underlying sentiment in the surrounding words.\n","By fine-tuning BERT on a sentiment-labeled dataset, it can be trained to classify texts into various sentiment categories, providing highly accurate and context-aware results. In this tutorial, we will explore how BERT can be fine-tuned for sentiment analysis tasks, helping to build models that can assess customer reviews, social media posts, and more."],"metadata":{"id":"rNQhV9tTVO2E"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"WmIXWglzVnKh"}},{"cell_type":"markdown","source":["**Exercise 11: Sentiment Analysis with BERT**  \n","\n","1. Import the model *\"nlptown/bert-base-multilingual-uncased-sentiment\"* from Hugging Face.  \n","2. Use the model to perform sentiment analysis on the following sentences:  \n","   - *\"I love this product!\"*  \n","   - *\"I hate this product!\"*  \n","3. Analyze the model's output for each sentence.  \n","   - How does the model interpret the sentiment of these sentences?  \n","   - What is the format of the output, and how can you interpret the sentiment scores or labels?"],"metadata":{"id":"5SwRnzT5VpO3"}},{"cell_type":"code","source":[],"metadata":{"id":"6HHGby1h8FNG","executionInfo":{"status":"ok","timestamp":1733932791121,"user_tz":0,"elapsed":4,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 12: Sentiment Analysis with a Different Model**  \n","\n","1. Import the model *\"cardiffnlp/twitter-roberta-base-sentiment\"* from Hugging Face.  \n","2. Perform sentiment analysis on the same sentences:  \n","   - *\"I love this product!\"*  \n","   - *\"I hate this product!\"*  \n","3. Compare the results from this model with those obtained in Exercise 11.  \n","   - How does the sentiment analysis output differ between the two models?  \n","   - What is the format of the output from the *\"cardiffnlp/twitter-roberta-base-sentiment\"* model, and how can you interpret it?"],"metadata":{"id":"WQdVnOhIVs0b"}},{"cell_type":"code","source":[],"metadata":{"id":"TyYw9iNF8yIS","executionInfo":{"status":"ok","timestamp":1733932791121,"user_tz":0,"elapsed":4,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Question Answering"],"metadata":{"id":"EL-qxi7oaF6H"}},{"cell_type":"markdown","source":["QA systems involve providing precise answers to questions based on a given context, often in the form of a passage of text.\n","BERT's bidirectional approach allows it to understand the full context of a sentence or passage, making it highly effective for identifying and extracting relevant information. Unlike traditional models, which process text in a left-to-right or right-to-left fashion, BERT processes the entire context simultaneously, understanding the relationships between all words. This ability to capture subtle contextual nuances is crucial in QA tasks, where the answer is often hidden within complex or ambiguous phrasing.\n","In QA applications, BERT is typically fine-tuned on datasets such as the SQuAD (Stanford Question Answering Dataset), where it learns to pinpoint the correct span of text that answers a given question. BERT's performance on QA tasks has set new benchmarks, achieving state-of-the-art results by accurately identifying the answer's location in the context.\n","In this part of the tutorial, we will explore how BERT can be used for Question Answering tasks, from fine-tuning models to extracting answers from a given text, demonstrating its power in real-world applications."],"metadata":{"id":"TcO8X5rDWQ7b"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"OQZkOz57WRtu"}},{"cell_type":"markdown","source":["**Exercise 13: Building a Question Answering Function with BERT**  \n","\n","1. Create a function that accepts a question and a context as inputs.  \n","2. Use the model *\"bert-large-uncased-whole-word-masking-finetuned-squad\"* from Hugging Face to process these inputs.  \n","3. The function should output the answer to the question based on the context provided.  \n","4. Ensure the function uses the proper tokenization and model inference steps to generate the answer.  \n","\n","For example:  \n","- Input:  \n","  - **Question**: *\"What is the capital of France?\"*  \n","  - **Context**: *\"France is a country in Europe. Its capital is Paris, known for its culture and history.\"*  \n","- Output: *\"Paris\"*"],"metadata":{"id":"tvPQxEMfWlLf"}},{"cell_type":"code","source":["# Exemplo context\n","context = (\n","        \"\"\"Portugal, officially the Portuguese Republic, is a country in the Iberian Peninsula in Southwestern Europe.\n","    Featuring the westernmost point in continental Europe, to its north and east is Spain,\n","    with which it shares the longest uninterrupted border in the European Union;\n","    to the south and the west is the North Atlantic Ocean; and to the west and southwest lie the Macaronesian\n","    archipelagos of the Azores and Madeira, which are two autonomous regions of Portugal.\n","    Lisbon is the capital and largest city, followed by Porto, which is the only other metropolitan area.\n","\n","    The western part of the Iberian Peninsula has been continuously inhabited since prehistoric times,\n","    with the earliest signs of settlement dating to 5500 BCE.[14] Celtic and Iberian peoples arrived in the first millennium BCE,\n","    with Phoenician and later Punic influence reaching the south during the same period.\n","    The region came under Roman control in the second century BCE, followed by a succession of Germanic peoples\n","    and the Alans from the fifth to eighth centuries CE. Muslims conquered most of the Iberian Peninsula in the eighth\n","    century CE, but were gradually expelled by the Christian Reconquista over the next several centuries.\n","    Modern Portugal began taking shape during this period, initially as a county of the Christian Kingdom of León in 868,\n","    and ultimately as an independent Kingdom with the Treaty of Zamora in 1143.[15]\"\"\"\n",")"],"metadata":{"id":"qaRqK3oOWrH0","executionInfo":{"status":"ok","timestamp":1733932791121,"user_tz":0,"elapsed":4,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rzj-WbioaGP2","executionInfo":{"status":"ok","timestamp":1733932791122,"user_tz":0,"elapsed":4,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 14: Testing the Question Answering Function**  \n","\n","1. Test the function you created in Exercise 13 by providing different questions and contexts.  \n","2. Try various types of questions, such as factual queries, location-based questions, or questions involving more complex contexts."],"metadata":{"id":"ARxiKdRDa_gw"}},{"cell_type":"code","source":[],"metadata":{"id":"LE2Q_2bIA9qK","executionInfo":{"status":"ok","timestamp":1733932791122,"user_tz":0,"elapsed":4,"user":{"displayName":"Zuil Filho","userId":"14731231616780016906"}}},"execution_count":1,"outputs":[]}]}